{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture Extraction System - LLM Server\n",
        "Run all cells in order. Copy the ngrok URL at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install flask pyngrok transformers torch accelerate opencv-python openai-whisper easyocr -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Paste your token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "ngrok.set_auth_token(\"YOUR_NGROK_TOKEN_HERE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "import cv2\n",
        "import whisper\n",
        "import easyocr\n",
        "import subprocess\n",
        "import os\n",
        "import tempfile\n",
        "import uuid\n",
        "import threading\n",
        "import numpy as np\n",
        "\n",
        "app = Flask(__name__)\n",
        "model = None\n",
        "tokenizer = None\n",
        "jobs = {}\n",
        "FRAME_RATE = 30\n",
        "\n",
        "def process_video_task(job_id, video_path):\n",
        "    try:\n",
        "        jobs[job_id][\"status\"] = \"processing\"\n",
        "        jobs[job_id][\"progress\"] = 10\n",
        "        jobs[job_id][\"message\"] = \"Extracting audio...\"\n",
        "\n",
        "        audio_path = video_path.replace(\".mp4\", \".wav\").replace(\".avi\", \".wav\").replace(\".mov\", \".wav\").replace(\".mkv\", \".wav\")\n",
        "        subprocess.run([\"ffmpeg\", \"-i\", video_path, \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", \"-y\", audio_path],\n",
        "                       stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
        "\n",
        "        jobs[job_id][\"progress\"] = 25\n",
        "        jobs[job_id][\"message\"] = \"Transcribing with Whisper...\"\n",
        "\n",
        "        whisper_model = whisper.load_model(\"base\")\n",
        "        result = whisper_model.transcribe(audio_path, verbose=False)\n",
        "        segments = result.get(\"segments\", [])\n",
        "        transcript = [{\"start\": s[\"start\"], \"end\": s[\"end\"], \"text\": s.get(\"text\", \"\").strip(), \"confidence\": 0.0} for s in segments]\n",
        "        duration = segments[-1][\"end\"] if segments else 0\n",
        "\n",
        "        jobs[job_id][\"progress\"] = 50\n",
        "        jobs[job_id][\"message\"] = \"Extracting frames...\"\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frames_data = []\n",
        "        frame_count = 0\n",
        "        saved = 0\n",
        "        ocr_reader = easyocr.Reader([\"en\"], gpu=True)\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if frame_count % FRAME_RATE == 0:\n",
        "                timestamp = frame_count / fps\n",
        "                results = ocr_reader.readtext(frame)\n",
        "                printed_text = []\n",
        "                handwritten_text = []\n",
        "                for (bbox, text, conf) in results:\n",
        "                    if conf >= 0.5:\n",
        "                        printed_text.append(text)\n",
        "                frames_data.append({\n",
        "                    \"timestamp\": timestamp,\n",
        "                    \"printed_text\": \" \".join(printed_text),\n",
        "                    \"handwritten_text\": \" \".join(handwritten_text),\n",
        "                    \"ocr_confidence\": 0.0\n",
        "                })\n",
        "                saved += 1\n",
        "                if saved % 10 == 0:\n",
        "                    jobs[job_id][\"progress\"] = 50 + int(40 * saved / max(1, int(cap.get(cv2.CAP_PROP_FRAME_COUNT) / FRAME_RATE)))\n",
        "                    jobs[job_id][\"message\"] = f\"OCR on frames... {saved} done\"\n",
        "            frame_count += 1\n",
        "        cap.release()\n",
        "\n",
        "        jobs[job_id][\"progress\"] = 95\n",
        "        jobs[job_id][\"message\"] = \"Finalizing...\"\n",
        "\n",
        "        jobs[job_id][\"result\"] = {\"transcript\": transcript, \"frames\": frames_data, \"duration\": duration}\n",
        "        jobs[job_id][\"status\"] = \"completed\"\n",
        "        jobs[job_id][\"progress\"] = 100\n",
        "        jobs[job_id][\"message\"] = \"Done\"\n",
        "\n",
        "        os.remove(video_path)\n",
        "        if os.path.exists(audio_path):\n",
        "            os.remove(audio_path)\n",
        "    except Exception as e:\n",
        "        jobs[job_id][\"status\"] = \"failed\"\n",
        "        jobs[job_id][\"error\"] = str(e)\n",
        "        jobs[job_id][\"message\"] = str(e)\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    return jsonify({\"status\": \"healthy\", \"model_loaded\": model is not None}), 200\n",
        "\n",
        "@app.route('/upload', methods=['POST'])\n",
        "def upload_video():\n",
        "    try:\n",
        "        if 'video' not in request.files:\n",
        "            return jsonify({\"error\": \"No video file\"}), 400\n",
        "        file = request.files['video']\n",
        "        if file.filename == '':\n",
        "            return jsonify({\"error\": \"No file selected\"}), 400\n",
        "        job_id = str(uuid.uuid4())\n",
        "        video_path = os.path.join(tempfile.gettempdir(), f\"{job_id}_{file.filename}\")\n",
        "        file.save(video_path)\n",
        "        jobs[job_id] = {\"status\": \"processing\", \"progress\": 0, \"message\": \"Starting...\", \"result\": None, \"error\": None}\n",
        "        threading.Thread(target=process_video_task, args=(job_id, video_path)).start()\n",
        "        return jsonify({\"job_id\": job_id}), 200\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/status/<job_id>', methods=['GET'])\n",
        "def get_status(job_id):\n",
        "    if job_id not in jobs:\n",
        "        return jsonify({\"error\": \"Job not found\"}), 404\n",
        "    j = jobs[job_id]\n",
        "    return jsonify({\"status\": j[\"status\"], \"progress\": j[\"progress\"], \"message\": j[\"message\"], \"error\": j.get(\"error\")}), 200\n",
        "\n",
        "@app.route('/result/<job_id>', methods=['GET'])\n",
        "def get_result(job_id):\n",
        "    if job_id not in jobs:\n",
        "        return jsonify({\"error\": \"Job not found\"}), 404\n",
        "    if jobs[job_id][\"status\"] != \"completed\":\n",
        "        return jsonify({\"error\": \"Job not ready\"}), 400\n",
        "    return jsonify(jobs[job_id][\"result\"]), 200\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate():\n",
        "    try:\n",
        "        data = request.json\n",
        "        prompt = data.get('prompt', '')\n",
        "        context = data.get('context', '')\n",
        "        max_tokens = data.get('max_tokens', 500)\n",
        "        temperature = data.get('temperature', 0.7)\n",
        "\n",
        "        # Truncate context to fit Phi-2's 2048 token limit (leave room for prompt + response)\n",
        "        max_context_chars = 4000\n",
        "        if len(context) > max_context_chars:\n",
        "            context = context[:max_context_chars] + \"\\n\\n[Context truncated...]\"\n",
        "\n",
        "        full_prompt = f\"\"\"Based on the following lecture content, answer the question.\n",
        "\n",
        "Lecture Content:\n",
        "{context}\n",
        "\n",
        "Question: {prompt}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        answer = generated_text[len(full_prompt):].strip()\n",
        "\n",
        "        return jsonify({\n",
        "            \"text\": answer,\n",
        "            \"metadata\": {\n",
        "                \"prompt_length\": len(full_prompt),\n",
        "                \"tokens_generated\": len(outputs[0]) - len(inputs['input_ids'][0])\n",
        "            }\n",
        "        }), 200\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "def load_llm():\n",
        "    global model, tokenizer\n",
        "    model_name = \"microsoft/phi-2\"\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
        "    config.pad_token_id = tokenizer.pad_token_id\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, config=config, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "def start_server():\n",
        "    load_llm()\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"\\nðŸš€ Server is running!\")\n",
        "    print(f\"ðŸ“¡ Public URL: {public_url}\")\n",
        "    print(f\"Copy this URL to your local app\\n\")\n",
        "    app.run(port=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "start_server()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}